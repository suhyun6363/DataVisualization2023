# 빅데이터 분석
데이터 수집 - 데이터 준비 - 데이터 탐색 - 데이터 모델링 - 모델 학습
## Ch07. 통계분석
### 개념
+ 기술 통계(description statistics): 데이터의  특성을 나타내는 수치를 이용해 분석하는 기본적인 통계 방법
  - 평균(mean), 중앙값(median), 최빈값(mode)
  - 데이터를 대표하는 값을 찾을 수 있음
  - 표준편차와 사분위로 데이터가 어떻게 분표되는지 알 수 있음
+ 회귀분석(regression analysis): 독립 변수, x와 종속변수, y 간의 상호 연관성 정도를 파악하기 위한 분석 기법
  - <b>변수 간의 인과관계 분석</b>
  - 독립 변수와 종속 변수의 관계에 따라 선형 회귀 분석과 비선형 회귀 분석으로 나눠짐
  - 결과 시각화: 히스토그램, 부분 회귀 플롯(하나의 독립 변수가 종속 변수에 미치는 영향력을 시각화)
+ 상관분석(correlation analysis): 변수 사이의 관계를 표현하고 수치를 보고자 하는 것입니다. 즉, 두 변수의 관계가 어떻고, 얼마나 강한 연결관계를 가지는 가를 수치, 척도로 표현하여 분석하는 방법
  - <b>상관계수는 인과 관계를 설명하지 않는다.</b>
  - 결과 시각화: 산점도, 히트맵
+ t-검정
  - 데이터에서 찾은 평균으로 두 그룹에 차이가 있는지 확인하는 방법
  - 30개 이하의 비교적 적은 수의 표본에 대해 활용한다.
+ 히스토그램
  - 데이터 값의 범위를 몇 개 구간으로 나누고 각 구간에 해당하는 값의 숫자나 상대적 빈도 크기를 차트로 나타낸 것
 ### 데이터 탐색
 - `info()` : 기본 정보 확인(샘플 개수, 속성 개수, 이름, 타입)
 - Pandas의 `describe()`, `unique()`: 유일한 값 출력, `value_counts()`: 속성값 빈도수
 ### 데이터 모델링
 1. `describe()` 함수로 그룹 비교하기
 ```
    wine.groupby('type')['quality'].describe()
 ```
 2. t-검정을 사용하여 그룹 간 차이 확인
  - P-Value 는 데이터가 우연에 의해 일어났을 확률이다. 
  - P-Value 가 유의수준(통상 5% ) 과 같거나 적다면 두 모집단이 유의미한 차이가 있다고 생각한다. 
 3. 회귀 분석을 수행하여 그룹 비교
 - 선형 회귀 분석
    + R-squared [R(결정계수, 선형회귀분석의 성능 평가 척도)] : 독립변수가 종속변수를 얼마나 잘 설명하는 지를 나타낸다. 1에 가까울수록 잘 설명함, 만들어진 회귀 직선이 현재 데이터셋을 얼마나 대표하는지
    + <b>선형관계의 척도로는 독립변수와 종속변수에는 인과관계를 설명할 수 없고 단지 독립변수의 변동이 종속변수의 변동을 어느정도를 설명하는지 나타낸다.</b>
    + 살제값과 예측값의 오차에 가반한 성능 평가 지표 사용: MAE, MSE, RMSE, R-squared
## Ch10. 회귀 분석
회귀 분석은 입력 데이터를 기반으로 결과를 예측하는 것, 가격, 주가, 수요, 날씨 등을 예측하는 데 많이 사용된다.
+ 머신러닝: 입력값을 받아 스스로 패턴을 찾아 학습하여 학습 모델을 생성, 그리고 새로운 값을 학습 모델에 입력하면 모델을 수행하여 분석값 또는 예측값을 출력한다.
  - 독립변수를 feature, 종속변수는 target
+ 머신러닝 프로세스: 데이터 수집 -> 데이터 전처리 및 훈련/테스트 데이터 분할 -> 모델 구축 및 학습 -> 모델 평가 -> 예측 
+ 지도 학습: 학습을 하기 위한 훈련 데이터에 입력과 출력을 같이 제공하므로 입력에 대한 결과값을 아는 상태에서 학습하는 방식, 회귀와 분류
+ 사이킷런: 파이썬 머신러닝 라이브러리, 독립변수 X, 데이터셋 객체의 data배열  종속변수 Y, 데이터셋 객체의 target 배열
## Ch11. 분류 분석
+ classification는 대표적인 지도학습 유형의 머신러닝 기법
+ logistic regression은 linear regression을 참/거짓의 이진 분류에 적용한 기법
  - 이진 분류 결과를 평가하기 위해 오차 행렬에 기반한 성능평가 지표 사용: 정밀도 재현율, F1 스코어, ROC_AUC
    - 오차행렬: 행은 실제 클래스의 Negative/Positive 값, 열은 예측 클래스의 Negative/Positive 값
    - 정확도: 예측값이 동일한 건수(TN+TP) / 전체데이터수
    - 정밀도: 예측 성능을 더 정밀하게 평가하기 위한 지표
    - 재현율: 실제 Positive인 데이터를 정확히 예측했는지 평가하는 지표
    - F1 스코어: 정밀도와 재현율이 서로 상충 관계인 문제점을 고려하여 정확한 평가를 위해 사용
    - ROC 기반 AUC 1에 가까울수록 좋은 성능
 + 다중 분류 문제는 결정 트리를 사용한다.
    - 결정 트리: 스스로 데이터 안에서 if/else 기반으로 규칙을 찾아 학습하여 트리 구조의 분류 규칙을 만듦
    - 분할된 하위 그룹에 있는 데이터의 균일도를 최대로 높게 만드는 규칙을 찾아서 규칙 노드로 구성해야 한다.
      + 결정트리는 정보이득지수(혼잡도가 줄어들어 얻게 되는 이득)가 높은 피처를 분할 기준으로 사용
      + 결정 트리에서는 지니 계수가 낮은(균일도가 높은 피처=혼잡도 낮음, 엔트로피 낮음) 피처를 분할 기준으로 사용
    - 결정 트리 모델의 하이퍼 매개변수를 수정하면 정확도를 높일 수 있다.
      + 노드를 분할하기 위한 최소 샘플 데이터 개수
      + 리프 노드가 되기 위한 최소 샘플 데이터 개수
      + 리프 노드에 들어가는 샘플 데이터의 최대 개수
      + 최적의 분할을 위해 고려할 최대 피처 개수
      + 트리의 최대 높이
    - RandomForest를 이용하여 성능을 높일 수 있다. -> 개별 나무 간 상관성을 줄여 예측력을 높일 수 있다.
## Ch12. 군집 분석
+ 비지도학습: 정답이 없어 학습을 통하여 데이터 안에 숨겨진 패턴을 찾아 타깃값을 만들어야 함, 대부분 군집 분석을 할 때 이용한다. 그리고 새로운 데이터의 특성을 분석하여 해당하는 클러스터를 예측한다.
+ K-means 알고리즘: 임의의 중심점을 잡고 기준으로 가까이 있는 데이터를 확인한 뒤 그들과의 거리의 평균 지점으로 중심점을 이동-> 과정 반복
-> 더 이상 이동이 발생하지 않는 위치를 찾으면 각 중심점을 기준으로 k개의 클러스터가 구성된다. but, k를 직점 구성해야 하는 문제가 있다.
+ 엘보방법: 왜곡은 클러스터의 중심점과 __클러스터 내의 데이터__ 거리 차이의 제곱값 합이라 하고 클러스터 개수 k의 변화에 따른 왜곡의 변화를 그래프로 나타내어 그래프의 꺾이는 지점의 k를 최적의 k로 선택한다.
+ 실루엣 분석: 클러스터 내에 있는 데이터가 얼마나 조밀하게 모여있는지를 측정하는 그래프 도구, 클러스터 응집력(해당 데이터의 클러스터 내의 데이터와 얼마나 가까운지)와 클러스터 분리도(가장 가까운 다른 클러스터 내의 데이터와 얼마나 떨어져있는가)를 이용하여 실루엣 계수를 계산한다.
